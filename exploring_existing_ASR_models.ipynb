{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Speech Recognition Mini-Project\n",
        "This project explores existing models available on Hugging Face for ASR and accent classification including:\n",
        "1. Use Whisper (seq2seq architecture) English checkpoint to transcribe speech\n",
        "2. Use wav2vec2 (CTC architecture) to extract phonemic transcription from English speech and compare results from 4 different models\n",
        "3. Use audio classification model for accent recognition to extract accent group from speech"
      ],
      "metadata": {
        "id": "NAaMjO7y9Q5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up common code for all models"
      ],
      "metadata": {
        "id": "u9dK7ltsfM60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some models want me to log into hugging face\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "WOx14bDrfMHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import IPython.display as ipd\n",
        "import torch"
      ],
      "metadata": {
        "id": "qAqbuuA8fRtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sample audio to test models\n",
        "convo, sr = librosa.load(\"convo.wav\", sr=16000)"
      ],
      "metadata": {
        "id": "VKaAJPHDfUSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ipd.Audio(\"convo.wav\") # listen to file in notebook"
      ],
      "metadata": {
        "id": "h3iy18U8fYhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "CdKbhNObfYaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper English Checkpoint to Transcribe speech"
      ],
      "metadata": {
        "id": "qcLeFxDdDYf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up pipeline to do ASR using whisper-small\n",
        "model = \"openai/whisper-base.en\"\n",
        "# model = \"openai/whisper-small.en\" # try this one if I need better accuracy\n",
        "\n",
        "# load the model in half-precision (float16) if running on a GPU to speed up inference\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "    torch_dtype = torch.float16\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    torch_dtype = torch.float32\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "id": "F3zqUHlM95hV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function that takes a filepath for audio input and pipeline loads audio, resamples it,\n",
        "# runs inference with the model and returns transcribed text\n",
        "def transcribe_speech(filepath):\n",
        "    output = pipe(\n",
        "        filepath,\n",
        "        max_new_tokens=256,\n",
        "        chunk_length_s=30,\n",
        "        batch_size=8,\n",
        "    )\n",
        "    return output[\"text\"]\n",
        "  # generate_kwargs={\"task\": \"transcribe\",\"language\": \"en\",},"
      ],
      "metadata": {
        "id": "94KLsbF0-u-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gradio"
      ],
      "metadata": {
        "id": "PvkS6M6b-5lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with individual file\n",
        "result = transcribe_speech('convo.wav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpDMGxBleyV9",
        "outputId": "96e6ecc1-9f8b-47cf-fd32-40ae3d4b57d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/automatic_speech_recognition.py:312: FutureWarning: `max_new_tokens` is deprecated and will be removed in version 4.49 of Transformers. To remove this warning, pass `max_new_tokens` as a key inside `generate_kwargs` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "D08QFNJHhQTD",
        "outputId": "d7a10162-a906-4cb2-e34b-e7d9b4068516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hey, what do you want to do today? Could we go to the beach? Is the weather nice? Yeah, that's a great idea. Let's bring tortilla. It's sunny. Sounds good. She could use some exercise.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wav2vec2 to extract phonemic transcription from English speech\n",
        "Compare 4 wav2vec2 models that output phonemic transcriptions but were trained on different datasets"
      ],
      "metadata": {
        "id": "ALDof_FQDf_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phoneme_models = [\n",
        "    \"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\", # Trained on DARPA TIMIT American English\n",
        "    \"mrrubino/wav2vec2-large-xlsr-53-l2-arctic-phoneme\", # Trained on speakers of English as a second langauge\n",
        "    \"vitouphy/wav2vec2-xls-r-300m-phoneme\", # Trained on unknown dataset\n",
        "    \"ct-vikramanantha/phoneme-scorer-v2-wav2vec2\", # Trained on LJSpech which sounds like Americans reading text in English\n",
        "]\n",
        "pipes = []"
      ],
      "metadata": {
        "id": "v_cXy8gadB1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, model in enumerate(phoneme_models):\n",
        "  pipe = pipeline(\"automatic-speech-recognition\", model=model)\n",
        "  pipes.append(pipe)"
      ],
      "metadata": {
        "id": "Ki6y-xArdF-E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = []\n",
        "for i, pipe in enumerate(pipes):\n",
        "  output = pipe(convo.copy())\n",
        "  outputs.append(output)"
      ],
      "metadata": {
        "id": "XdCfoKzDcv09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, output in enumerate(outputs):\n",
        "  print(str(i+1) + \": \" + str(output['text']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHr56TBiS9Ug",
        "outputId": "c352eb6e-26d8-44ef-d470-91f4f4dfee98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: heɪwəɾiwənɪ duɾɪdeɪkʊwiʊ tɪðə bi ʧɪzðəwɛðɝnaɪsjæðɛ szɪ gɹeɪɾaɪ diə lɛ s b ɹɪŋ tɝ tiə ɪ səni saʊn z gʊ ʃi kʊɾjusəmɛ k sɝsaɪz\n",
            "2: hei wʌt ju wɑndʌ duɪ tʌ deɪ kʊd wi ɡoʊt tʌ ðʌ bit͡ʃ ɪz ðʌ wɛðɚnaɪsjæ ðæts ʌ ɡɹeɪd aɪdiʌ lɛts pɹɪŋ tʌ  diʌ ɪts sʌni sʌʊmz ɡɛt ʃi kɹud sʌm ɛksɚsaɪs\n",
            "3: h#hheywahdywahn dahduwtihdeyh#kwiygow tahdhahbiychh#ihzdhahwehdhernaysyehdhaet sahg reytaydiyahh#leht s b rihngtertiyahh#iht sahniyh#sawn z gershiykeryuwz sahmehk sersayzh#\n",
            "4: hay u n y oo w n i d t aw k uoh d w ee goht aw bth aw v eechi z bthohw e bth or nIs bth a t s bth g raytI ee aw l e t s b r i ng t or d ee  i t th u n ee  sown g e sh ee k uoh d y oo z s u m e k s or sIz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# These are pretty different outputs\n",
        "# Models 3-4 look like they're outputting non-IPA output, so that's probably not what I want, I'd like IPA phonemic transcriptions\n",
        "# Model 1-2 appear to be using IPA but look rather different still, with different spacing decisions, and different symbols\n",
        "# Let's more deeply compare model 1 and 2 output below"
      ],
      "metadata": {
        "id": "jBRjXJ4TjhsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing spacing to line up sound with the transcript and expected phonemes\n",
        "# 0 = transcript\n",
        "# 1 = output of model 1 from audio input\n",
        "# 2 = output of model 2 from audio input\n",
        "# 3 = output from ChatGPT of expected phonemes given the text, assuming spoken in standard colloquial American English\n",
        "\n",
        "# 0: Hey what do you wanna  do  today?\n",
        "# 1: heɪ wəɾ  i      wənɪ   du  ɾɪdeɪ\n",
        "# 2: hei wʌt  j  u   wɑndʌ  duɪ tʌdeɪ\n",
        "# 3: heɪ wʌɾə jə     wɑnə   du  təˈdeɪ\n",
        "\n",
        "# 0: Could we  go   to    the beach?\n",
        "# 1: kʊ    wi  ʊ    tɪ    ðə  biʧ\n",
        "# 2: kʊd   wi  ɡoʊ  t tʌ  ðʌ  bit͡ʃ\n",
        "# 3: kʊd   wi  ɡoʊ  tə    ðə  biʧ\n",
        "\n",
        "# 0: Is   the weather nice?\n",
        "# 1: ɪz   ðə  wɛðɝ    naɪs\n",
        "# 2: ɪz   ðʌ  wɛðɚ    naɪs\n",
        "# 3: ɪz   ðə ˈwɛðɚ    naɪs\n",
        "\n",
        "# 0: Yeah that's a  great  idea,\n",
        "# 1: jæ   ðɛ szɪ    gɹeɪɾ  aɪdiə\n",
        "# 2: jæ   ðæts   ʌ  ɡɹeɪd  aɪdiʌ\n",
        "# 3: jɛ   ðæts   ə  ɡreɪɾ  aɪˈdiə\n",
        "\n",
        "# 0: lets bring Tortilla. It's sunny.\n",
        "# 1: lɛ s bɹɪŋ  tɝ tiə    ɪ    səni\n",
        "# 2: lɛts pɹɪŋ  tʌ  diʌ   ɪts  sʌni\n",
        "# 3: lɛts brɪŋ  tɔɹˈtiʝə  ɪts ˈsʌni\n",
        "\n",
        "# 0: Sounds good, she could use some exercise.\n",
        "# 1: saʊnz  gʊ    ʃi  kʊɾj  u   səm  ɛksɝsaɪz\n",
        "# 2: sʌʊmz  ɡɛt   ʃi  kɹud      sʌm  ɛksɚsaɪs\n",
        "# 3: saʊnz  ɡʊd   ʃi  kəd   juz səm ˈɛksɚˌsaɪz\n",
        "\n",
        "# Overall, Models 1-2 differ in a lot of sounds, and sometimes it appears Model 1 missed a few\n",
        "# sounds that Model 2 picked up on, but to be fair, the speakers are both New Englanders who tend\n",
        "# to soften certain consonants like 't' and 'd'\n",
        "\n",
        "# Since Model 1 was trained on American accents, while model 2 was trained on speakers of English as a\n",
        "# second language, we'd expect model 1 to be more accurate on this native Massachusetts speakers conversation\n",
        "\n",
        "# I think the best idea would be to do an accent detection algorithm, and if the speaker sounds American,\n",
        "# send the audio to model 1, but if the speaker sounds like English is their second language, send the audio\n",
        "# to model 2.  Both will output similar IPA phonemic transcripts, but model 2 might capture non-standard American\n",
        "# pronunciation better"
      ],
      "metadata": {
        "id": "DV00a8oGzA2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore accent classifier for native-English speakers"
      ],
      "metadata": {
        "id": "MZ4QmivhdtsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing out a preexisting accent classification model trained on SpeechBrain CommonAccent dataset which includes 16 accents from recordings in English including:\n",
        "\n",
        " - African\n",
        " - Australian\n",
        " - Bermudan\n",
        " - Canadian\n",
        " - English\n",
        " - Hong Kong\n",
        " - Indian\n",
        " - Ireland\n",
        " - Malaysian\n",
        " - New Zealand\n",
        " - Philippines\n",
        " - Scotland\n",
        " - Singapore\n",
        " - South Atlantic\n",
        " - US\n",
        " - Whales\n",
        "\n",
        "** Notably missing are English speakers whose first language is Spanish, which is a major limitation of this dataset and related models for use in the US"
      ],
      "metadata": {
        "id": "E-dkC5lUMXTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install speechbrain"
      ],
      "metadata": {
        "id": "122ALelwN_bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from speechbrain.pretrained import EncoderClassifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX9RFO-gOCbW",
        "outputId": "72e50904-63da-4da0-85d2-fb48d7037e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-df014b53a41a>:2: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
            "  from speechbrain.pretrained import EncoderClassifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = EncoderClassifier.from_hparams(source=\"Jzuluaga/accent-id-commonaccent_ecapa\", savedir=\"pretrained_models/accent-id-commonaccent_ecapa\")"
      ],
      "metadata": {
        "id": "rVuUAsitOCUq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test on a bunch of accents, some native English from outside US, some not\n",
        "# american: clip from Reservation Dogs, a show with Indigenous/Native American actors\n",
        "# irish: clip from Derry Girls, an Irish TV show\n",
        "# indian: Abdul Bari, and Indian professor, on YouTube teaching algorithms\n",
        "# mexican: Jaime Camil, Mexican actor from Jane the Virgin\n",
        "# south_african: Trevor Noah US-based comedian born in South Africa\n",
        "# chinese: Ronny Chieng, Chinese-American comedian\n",
        "# nigerian: Daniel Etim Effiong and Tana Adelana in Dinner for Four, a Nigerian Film\n",
        "accents = ['american', 'irish', 'indian', 'mexican', 'south_african', 'chinese', 'nigerian']\n",
        "predicted_accents = []\n",
        "likelihoods = []\n",
        "for i, accent in enumerate(accents):\n",
        "  out_prob, score, index, text_lab = classifier.classify_file(accent+'.wav')\n",
        "  predicted_accents.append(text_lab[0])\n",
        "  likelihoods.append(float(score[0]*100))"
      ],
      "metadata": {
        "id": "7vptr6biPEJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, accent in enumerate(accents):\n",
        "  print(f\"The {accent} recording was classified as {predicted_accents[i]} with probability {likelihoods[i]:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbHj83JfO_kE",
        "outputId": "3036796d-7747-4b30-97cc-b54e72f35d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The american recording was classified as us with probability 81.86%\n",
            "The irish recording was classified as england with probability 68.69%\n",
            "The indian recording was classified as indian with probability 67.83%\n",
            "The mexican recording was classified as australia with probability 58.15%\n",
            "The south_african recording was classified as us with probability 56.03%\n",
            "The chinese recording was classified as us with probability 62.65%\n",
            "The nigerian recording was classified as england with probability 67.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, we can see the classifier is not very accurate.\n",
        "\n",
        "Correct: American and Indian\n",
        "\n",
        "Incorrect (but expected given these speakers are not represented in training data): Mexican, Chinese\n",
        "\n",
        "Incorrect unexpectedly: Irish, Nigerian\n",
        "\n",
        "*Note: South African speaker in this recording is Trevor Noah, who is from South Africa but has been in the US for a while, so we can give the model a pass on that"
      ],
      "metadata": {
        "id": "q_3V_NbcQoaj"
      }
    }
  ]
}