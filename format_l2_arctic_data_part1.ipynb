{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05035cf4-12db-4476-b1e4-6730b247f2bf",
   "metadata": {},
   "source": [
    "# Accent Classification Project: Data Prep Part 1\n",
    "This file is 2 of 3 in an Accent Classification Project\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "The goal of this project is to create an accent classifier for people who learned English as a second language by fine-tuning a speech recognition model to classify accents from 24 people speaking English whose first language is Hindi, Korean, Arabic, Vietnamese, Spanish, or Mandarin.\n",
    "\n",
    "**Data source**\n",
    "\n",
    "https://psi.engr.tamu.edu/l2-arctic-corpus/\n",
    "\n",
    "L2-Arctic dataset comes via email and includes approximately 24-30 hours of recordings where 24 speakers read passages in English. The first languages of the speakers are Arabic, Hindi, Korean, Mandarin, Spanish, and Vietnamese.  There's 2 women and 2 men in each language group.\n",
    "\n",
    "The original dataset is around 8GB with contains 27,000 rows of data, each with an audio file of 3-4s with 48k Hz sampling rate.\n",
    "\n",
    "**Summary of this file**\n",
    "\n",
    "This file reformats the original L2-Arctic data for distilHuBERT model by splitting the file in 6 smaller pieces, one for each language group. The number of files per speaker is limited to 560 to use approximately half of the original data. Thus each piece is about 0.66GB with 2,240 rows. \n",
    "For each language group file, the wav is loaded, resampled to 16,000 Hz, and rows are then combined so the audio's are up to 30s long, as expected by distilHuBERT model.  This reduced the number of rows to about 300 in each language group. Then the reformatted data is wrapped in the Hugging Face dataset class (most memory-intensive step) and saved to disk. \n",
    "\n",
    "**Result**\n",
    "\n",
    "6 Hugging Face datasets (one for each language group) with about 300 rows. Each row contains the label for the language group and an audio file of 30 seconds or less at 16k Hz.\n",
    "\n",
    "**Environment**\n",
    "\n",
    "It runs best on a mac CPU, which is faster than google colab's CPU or GPU.  Even when code is re-written to process files in bulk with GPU, a mac CPU is still surprisingly much faster, and splitting the dataset into smaller pieces avoids memory problems.  Run file on a language group, save to disk, then shut down and re-start the Jupyter Notebook server to clear working memory before starting the next language group.\n",
    "\n",
    "**Data source**\n",
    "\n",
    "The [L2-Arctic](https://psi.engr.tamu.edu/l2-arctic-corpus/) data is ~8GB and comes via email. It includes approximately 24-30 hours of recordings where 24 speakers read passages in English. The first languages of the speakers are Arabic, Hindi, Korean, Mandarin, Spanish, and Vietnamese.  There's 2 women and 2 men in each language group.\n",
    "\n",
    "**Foundation Model**\n",
    "\n",
    "[DistilHuBERT](https://huggingface.co/ntu-spml/distilhubert) is a smaller version of HuBERT that was modified from BERT. BERT is a speech recognition model with encoder-only CTC architecture.  For this project, a classification layer was added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5f0d0-8ba5-4b64-b0f2-0e34f8a12b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mData\u001b[m\u001b[m         \u001b[34mLaCie\u001b[m\u001b[m        \u001b[35mMacintosh HD\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# Check to make sure Jupyter Notebook can access my external drive where the data is saved\n",
    "!ls /Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b406dc8-7140-4a5b-8831-08fc41816bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72bbce95-fc0f-45fc-a83f-12609deee399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dontfreakout/.pyenv/versions/3.12.8/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae65dc4-d7e5-4df1-8a9d-e85cb0c68d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parent directory\n",
    "parent_dir = \"/Volumes/LaCie/l2-arctic-data/arctic/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7475e89a-4518-49b4-94de-359879176293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over wav files in each speaker folder and create dataset in format {['speaker': 'ABA', 'file_path': 'drive...wav']}\n",
    "# Limit number of files per speaker, there's about 1122 files per speaker in total so 560 is about half\n",
    "num_files_per_speaker = 560\n",
    "\n",
    "### Change language here ###\n",
    "language = 'Vietnamese'\n",
    "\n",
    "data = []\n",
    "# speakers are defined here: https://psi.engr.tamu.edu/l2-arctic-corpus/\n",
    "arabic_speakers = ['ABA', 'SKA', 'YBAA', 'ZHAA']\n",
    "mandarin_speakers = ['BWC', 'LXC', 'NCC', 'TXHC']\n",
    "hindi_speakers = ['ASI', 'RRBI', 'SVBI', 'TNI']\n",
    "korean_speakers = ['HJK', 'HKK', 'YDCK', 'YKWK']\n",
    "spanish_speakers = ['EBVS', 'ERMS', 'MBMPS', 'NJS']\n",
    "vietnamese_speakers = ['HQTV', 'PNV', 'THV', 'TLV']\n",
    "\n",
    "### Change speakers here ###\n",
    "for speaker in vietnamese_speakers:\n",
    "  file_paths = glob.glob(os.path.join(parent_dir, speaker, \"wav\", \"*.wav\"))\n",
    "  #for file_path in file_paths: # use if using whole dataset\n",
    "  for file_path in file_paths[:num_files_per_speaker]:\n",
    "    dict = {'file_path': file_path, 'label': language}\n",
    "    data.append(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6293d4-c422-47c9-b899-a69ef005c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap in to Hugging Face dataset class\n",
    "data = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb0e3f8-abed-4db2-956e-dbcea2f8d1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file_path', 'label'],\n",
       "    num_rows: 2240\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4ffab79-7662-41c7-bffe-5ae393dda93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '/Volumes/LaCie/DELETE_Apr2025_l2-arctic-data/arctic/HQTV/wav/arctic_b0126.wav',\n",
       " 'label': 'Vietnamese'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7907842a-e8b8-4295-abd4-88719577898c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see features\n",
    "data.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605724f-7e29-4683-bde2-66998dfe5181",
   "metadata": {},
   "source": [
    "## Load and resample audio for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121688c5-33fe-4609-b395-5f47ce12549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose pretrained model DistilHuBERT which is a smaller version of HuBERT\n",
    "# Alternatively could try full HuBERT or Wav2Vec2 but these will take longer to train\n",
    "# HuBERT and Wav2Vec2 models take in raw audio, not spectrograms\n",
    "# https://huggingface.co/ntu-spml/distilhubert\n",
    "model_id = \"ntu-spml/distilhubert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed033e44-1268-45a8-8429-3b722715a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the AutoFeatureExtractor for DistilHuBERT so we can format data in way that model expects\n",
    "from transformers import AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06388484-7c60-4df4-89d9-93576e2678a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_id, do_normalize=True, return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80b4ec17-6404-4b9e-8583-d186f73baa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the sampling rate in the feature_extractor to see what SR the model expects\n",
    "SR = feature_extractor.sampling_rate\n",
    "SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adab7bd5-f8b2-4cf2-b430-003cb3a4c557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the sr used in dataset\n",
    "waveform, original_sr = torchaudio.load(data[0]['file_path'], normalize=True)\n",
    "original_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed8b92c2-36b3-4473-a36b-3e89ee273c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distilHuBERT expects audio clips to be exactly 30 seconds\n",
    "# This dataset has audio's that are only 3-4 seconds\n",
    "# Training could be made more efficient if audios from the same participant were appended to one another\n",
    "# until they are 30 seconds or slightly less\n",
    "# rather than padding a 3 second audio with 27 seconds of silence\n",
    "MAX_DURATION = 30.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1207c6-5591-43c0-a8a5-57327213d9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the max number of samples\n",
    "MAX_SAMPLES=int(SR * MAX_DURATION)\n",
    "MAX_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337fba8-7077-4ccc-ac1e-3bb2cf1eefc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 500 rows\n",
      "completed 1000 rows\n",
      "completed 1500 rows\n",
      "completed 2000 rows\n",
      "saving last row with this many samples: 405404\n"
     ]
    }
   ],
   "source": [
    "# Use for CPU only\n",
    "# Load, resample, and combine for 30s in 1 function for efficiency\n",
    "def download_resample_and_merge_audio(dataset):\n",
    "    tracker = 0\n",
    "    grouped_data = []  # Stores the final dataset\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=original_sr, new_freq=SR)\n",
    "    new_audio = []\n",
    "\n",
    "    for row in dataset:\n",
    "        file_path = row[\"file_path\"]\n",
    "        waveform, sr = torchaudio.load(file_path, normalize=True)\n",
    "        resampled_waveform = resampler(waveform)\n",
    "        audio = {\"audio\": resampled_waveform.numpy()[0]}\n",
    "        \n",
    "        audio_samples = audio[\"audio\"]\n",
    "\n",
    "        # Check if adding this row exceeds max limit\n",
    "        if len(new_audio) + len(audio_samples) <= MAX_SAMPLES:\n",
    "            new_audio.extend(audio_samples)\n",
    "        else:\n",
    "            # Save the current row and start a new row\n",
    "            grouped_data.append({\"label\": language, \"audio\": new_audio})\n",
    "            new_audio = []\n",
    "            new_audio.extend(audio_samples)\n",
    "\n",
    "        # Print a tracker to watch progress\n",
    "        tracker = tracker+1\n",
    "        if tracker % 500 == 0:\n",
    "            print('completed ' + str(tracker) + ' rows')\n",
    "\n",
    "    \n",
    "    # Save remaining data that didn't exceed MAX_SAMPLES\n",
    "    if len(new_audio) > 0:\n",
    "        print('saving last row with this many samples: ' + str(len(new_audio)))\n",
    "        grouped_data.append({\"label\": language, \"audio\": new_audio})\n",
    "    return grouped_data\n",
    "    \n",
    "\n",
    "data3 = download_resample_and_merge_audio(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32b834a5-94d4-4c13-a364-4eb8cca2955b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data format\n",
    "# data3\n",
    "# data3[0]\n",
    "len(data3) # Check length of dataset after rows were combined\n",
    "# len(data3[0]['audio']) # check length of an individual audio - should be just less than 480,000\n",
    "# len(data3[len(data3)-1]['audio']) # check the length of the last audio row in the dataset, should be a lot less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68e9534d-88a6-4477-96c6-b596fdd89ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "data4 = Dataset.from_list(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc1671-d87c-45b4-bf4e-39fcda43a5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-check data format\n",
    "# data4\n",
    "# data4[0]\n",
    "len(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70550f-82da-4253-b40a-f0f507a062ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|â–ˆ| 307/307 [00:05<00:00, 53.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# save hugging face dataset back to disk\n",
    "data4.save_to_disk(\"/Volumes/LaCie/l2-arctic-data/\"+language)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
